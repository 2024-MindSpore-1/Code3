{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6486858d-9588-4bdc-9879-e06a2b76e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daiyuxin/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindnlp/utils/download.py:29: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39b322b0b5c4045a14ccdddb2fa534d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/454M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(1911869:140261362038592,MainProcess):2023-09-19-23:08:00.170.668 [mindspore/lib/python3.9/site-packages/mindnlp/abc/models/pretrained_model.py:457] The following parameters in checkpoint files are not loaded:\n",
      "['cls.predictions.decoder.weight']\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.models import BertForPretraining\n",
    "\n",
    "import mindspore as ms\n",
    "\n",
    "model = BertForPretraining.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02a34e22-355c-4ea3-8edd-6911be35ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transforms import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0b88487-b91f-41ba-9c7f-5414733264ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dec0b577-b4f6-4677-9bf6-8b06d92209ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"两个警察在[MASK]喝酒，不一会儿进来了两个[T-MASK]，他们自称为[MASK]\"\n",
    "res = tokenizer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d716c48-3d93-4b2c-b161-c0235fc3746e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101,  697,  702, 6356, 2175, 1762,  103, 1600, 6983, 8024,  679,\n",
       "        671,  833, 1036, 6822, 3341,  749,  697,  702,  138,  100,  118,\n",
       "        100,  140, 8024,  800,  812, 5632, 4917,  711,  103,  102])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db06941f-d797-43df-9239-be6a93ea0a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.set_train(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3bab3a2-7fdd-42bb-bb84-036d5db4f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "727f628e-ef0c-4497-bb7c-70db1de275d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101,  697,  702, 6356, 2175, 1762,  103, 1600, 6983, 8024,  679,\n",
       "        671,  833, 1036, 6822, 3341,  749,  697,  702,  138,  100,  118,\n",
       "        100,  140, 8024,  800,  812, 5632, 4917,  711,  103,  102])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f5f80-551e-4b1b-a681-bfc48182b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mindnlp官方库暂未实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d58a4461-d8b8-403c-9da2-f40f17652cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m out \u001b[38;5;241m=\u001b[39m model(ms\u001b[38;5;241m.\u001b[39mTensor(res\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m      2\u001b[0m out \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindnlp/abc/transforms/pretrained_tokenizer.py:152\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindnlp/abc/transforms/pretrained_tokenizer.py:169\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 169\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     sub_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindnlp/abc/transforms/pretrained_tokenizer.py:266\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    264\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_decoder[index])\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_id_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m~/anaconda3/envs/kewei-mindspore/lib/python3.9/site-packages/mindnlp/abc/transforms/pretrained_tokenizer.py:270\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_id_to_token\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_id_to_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = model(ms.Tensor(res.reshape(-1,1)))\n",
    "out = out[0].argmax(-1).numpy()\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89782f1b-286c-4c2c-839d-506d7898d920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused506] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ， 不 一 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] t - mask [UNK] ， [UNK] [UNK] [UNK] [UNK] [UNK] [unused506] [UNK]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "input = \"两个警察在[MASK]喝酒，不一会儿进来了两个[T-MASK]，他们自称为[MASK]\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('./bert')\n",
    "res = tokenizer(input)\n",
    "out = model(ms.Tensor([res['input_ids']]))\n",
    "out = out[0].argmax(-1).numpy()\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "822e598c-5d82-42a1-bb90-2909605fbb28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(1911869:140261362038592,MainProcess):2023-09-19-23:18:48.436.897 [mindspore/lib/python3.9/site-packages/mindnlp/abc/models/pretrained_model.py:457] The following parameters in checkpoint files are not loaded:\n",
      "['cls.predictions.decoder.weight']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[unused506] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ， 不 一 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] t - mask [UNK] ， [UNK] [UNK] [UNK] [UNK] [UNK] [unused506] [UNK]'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindnlp.models import BertForPretraining\n",
    "\n",
    "import mindspore as ms\n",
    "\n",
    "model = BertForPretraining.from_pretrained('bert-base-chinese')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "input = \"两个警察在[MASK]喝酒，不一会儿进来了两个[T-MASK]，他们自称为[MASK]\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('./bert')\n",
    "res = tokenizer(input)\n",
    "out = model(ms.Tensor([res['input_ids']]))\n",
    "out = out[0].argmax(-1).numpy()\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2dab082b-a775-4479-8c11-408af686aa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(1911869:140261362038592,MainProcess):2023-09-19-23:20:45.284.117 [mindspore/lib/python3.9/site-packages/mindnlp/abc/models/pretrained_model.py:457] The following parameters in checkpoint files are not loaded:\n",
      "['cls.predictions.decoder.weight']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'， 两 个 警 察 在 外 喝 酒 ， 不 一 会 儿 进 来 了 两 个 人 ， 他 们 自 称 为 来 自 [ [UNK] - [UNK] ] 的 囚 犯 ， 并 声 称 他 们 是 冤 枉 的 。 他'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindnlp.models import BertForPretraining\n",
    "\n",
    "import mindspore as ms\n",
    "\n",
    "model = BertForPretraining.from_pretrained('bert-base-chinese')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "input = \"两个警察在[MASK]喝酒，不一会儿进来了两个[MASK]，他们自称为来自[T-MASK]的囚犯，并声称他们是冤枉的。\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('./bert_cn')\n",
    "res = tokenizer(input)\n",
    "out = model(ms.Tensor([res['input_ids']]))\n",
    "out = out[0].argmax(-1).numpy()\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80364306-a7f0-4a85-863b-29b257f60615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
