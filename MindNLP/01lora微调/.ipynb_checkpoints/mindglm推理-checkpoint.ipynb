{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd52af7-52b5-4040-b1f8-d6c94020a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import mindspore as ms\n",
    "import numpy as np\n",
    "import argparse\n",
    "from mindformers.models.glm import GLMConfig, GLMChatModel, GLMChatModelWithLora\n",
    "from mindformers.models.glm.chatglm_6b_tokenizer import ChatGLMTokenizer\n",
    "from mindformers.models.glm.glm_processor import process_response\n",
    "from mindformers.pet.pet_config import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae38766-db78-4220-9a8a-c93c07c248c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seq_length', default=1024, type=int, help='Which device to run service.')\n",
    "    parser.add_argument('--device_id', default=0, type=int, help='Which device to run service.')\n",
    "    parser.add_argument('--checkpoint_path', type=str, default='/home/ma-user/work/mindglm/mindformers/output/checkpoint/rank_0/glm-6b-lora_rank_0_1-network.ckpt', help='Checkpoint file to load on.')\n",
    "    parser.add_argument('--vocab_path', type=str, default='/home/ma-user/work/mindglm/checkpoint_download/glm/ice_text.model', help='Vocab file to load on.')\n",
    "    parser.add_argument('--is_lora', type=str, default='true',help='Whether is lora model.')\n",
    "    return parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44a89f2-25e9-40c7-b9b3-638edf4d3c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_arguments(['--device_id','0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e686f2e2-df16-4e6f-bf70-f66429d75f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.is_lora.lower() == \"true\":\n",
    "    is_lora = True\n",
    "else:\n",
    "    is_lora = False\n",
    "\n",
    "config = GLMConfig(\n",
    "    position_encoding_2d=True,\n",
    "    use_past=True,\n",
    "    is_sample_acceleration=True,\n",
    ")\n",
    "\n",
    "pet_config = LoraConfig(\n",
    "    lora_rank=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa2c1c7-72b7-4031-a39c-03455aa38de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\", device_id=args.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1631bfa-acec-4990-966f-8ddc2e72e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:08:27.679.054 [mindspore/common/_decorator.py:38] 'TensorAdd' is deprecated from version 1.1 and will be removed in a future version, use 'Add' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-09 16:09:51,614 - mindformers - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.\n",
      "2023-09-09 16:09:57,432 - mindformers - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.\n"
     ]
    }
   ],
   "source": [
    "if is_lora:\n",
    "    config.pet_config = pet_config\n",
    "    model = GLMChatModelWithLora(config)\n",
    "# else:\n",
    "#     model = GLMChatModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802cc642-5f20-4640-8e06-a54f05c29e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLMChatModelWithLora<\n",
       "  (transformer): GLMModel<\n",
       "    (embedding_dropout): Dropout<keep_prob=1.0>\n",
       "    (word_embeddings): VocabEmbedding<>\n",
       "    (layers): CellList<\n",
       "      (0): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (1): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (2): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (3): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (4): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (5): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (6): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (7): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (8): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (9): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (10): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (11): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (12): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (13): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (14): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (15): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (16): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (17): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (18): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (19): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (20): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (21): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (22): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (23): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (24): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (25): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (26): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      (27): DeepNormWithGLULayer<\n",
       "        (input_layernorm): LayerNorm<>\n",
       "        (attention): RotaryEmbeddingFP32SoftmaxSelfAttention<\n",
       "          (rotary_emb): RotaryEmbedding<>\n",
       "          (query_key_value): LoRADense<\n",
       "            input_channels=4096, output_channels=12288, has_bias=True\n",
       "            (lora_dropout): Dropout<keep_prob=0.9>\n",
       "            >\n",
       "          (attention_dropout): Dropout<keep_prob=1.0>\n",
       "          (dense): Linear<>\n",
       "          (output_dropout): Dropout<keep_prob=1.0>\n",
       "          (softmax): Softmax<>\n",
       "          >\n",
       "        (post_attention_layernorm): LayerNorm<>\n",
       "        (mlp): MLPWithGEGLU<\n",
       "          (activation_func): GELU<>\n",
       "          (dense_h_to_4h): Linear<>\n",
       "          (dense_4h_to_h): Linear<>\n",
       "          (dropout): Dropout<keep_prob=1.0>\n",
       "          >\n",
       "        >\n",
       "      >\n",
       "    (final_layernorm): LayerNorm<>\n",
       "    >\n",
       "  (lm_head): GLMHead<>\n",
       "  (loss): CrossEntropyLoss<\n",
       "    (_softmax): _Softmax<>\n",
       "    (_nllloss): _NLLLoss<>\n",
       "    >\n",
       "  (post_logits): ProcessLogits<>\n",
       "  >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00dd5d31-9d71-4269-8db3-6536c2e55dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.638.301 [mindspore/train/serialization.py:711] For 'load_param_into_net', 56 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.643.441 [mindspore/train/serialization.py:716] transformer.layers.0.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.644.402 [mindspore/train/serialization.py:716] transformer.layers.0.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.645.356 [mindspore/train/serialization.py:716] transformer.layers.1.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.646.253 [mindspore/train/serialization.py:716] transformer.layers.1.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.647.108 [mindspore/train/serialization.py:716] transformer.layers.2.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.647.923 [mindspore/train/serialization.py:716] transformer.layers.2.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.648.787 [mindspore/train/serialization.py:716] transformer.layers.3.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.649.632 [mindspore/train/serialization.py:716] transformer.layers.3.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.650.463 [mindspore/train/serialization.py:716] transformer.layers.4.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.651.255 [mindspore/train/serialization.py:716] transformer.layers.4.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.651.988 [mindspore/train/serialization.py:716] transformer.layers.5.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.652.857 [mindspore/train/serialization.py:716] transformer.layers.5.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.653.670 [mindspore/train/serialization.py:716] transformer.layers.6.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.657.788 [mindspore/train/serialization.py:716] transformer.layers.6.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.658.600 [mindspore/train/serialization.py:716] transformer.layers.7.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.659.339 [mindspore/train/serialization.py:716] transformer.layers.7.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.660.181 [mindspore/train/serialization.py:716] transformer.layers.8.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.660.983 [mindspore/train/serialization.py:716] transformer.layers.8.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.661.767 [mindspore/train/serialization.py:716] transformer.layers.9.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.662.495 [mindspore/train/serialization.py:716] transformer.layers.9.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.663.356 [mindspore/train/serialization.py:716] transformer.layers.10.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.664.213 [mindspore/train/serialization.py:716] transformer.layers.10.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.665.058 [mindspore/train/serialization.py:716] transformer.layers.11.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.665.844 [mindspore/train/serialization.py:716] transformer.layers.11.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.666.735 [mindspore/train/serialization.py:716] transformer.layers.12.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.667.537 [mindspore/train/serialization.py:716] transformer.layers.12.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.668.373 [mindspore/train/serialization.py:716] transformer.layers.13.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.669.215 [mindspore/train/serialization.py:716] transformer.layers.13.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.669.968 [mindspore/train/serialization.py:716] transformer.layers.14.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.670.810 [mindspore/train/serialization.py:716] transformer.layers.14.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.671.603 [mindspore/train/serialization.py:716] transformer.layers.15.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.672.395 [mindspore/train/serialization.py:716] transformer.layers.15.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.673.199 [mindspore/train/serialization.py:716] transformer.layers.16.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.674.030 [mindspore/train/serialization.py:716] transformer.layers.16.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.674.817 [mindspore/train/serialization.py:716] transformer.layers.17.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.675.657 [mindspore/train/serialization.py:716] transformer.layers.17.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.676.480 [mindspore/train/serialization.py:716] transformer.layers.18.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.677.337 [mindspore/train/serialization.py:716] transformer.layers.18.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.678.132 [mindspore/train/serialization.py:716] transformer.layers.19.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.678.930 [mindspore/train/serialization.py:716] transformer.layers.19.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.679.724 [mindspore/train/serialization.py:716] transformer.layers.20.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.680.496 [mindspore/train/serialization.py:716] transformer.layers.20.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.681.354 [mindspore/train/serialization.py:716] transformer.layers.21.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.682.159 [mindspore/train/serialization.py:716] transformer.layers.21.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.684.430 [mindspore/train/serialization.py:716] transformer.layers.22.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.685.429 [mindspore/train/serialization.py:716] transformer.layers.22.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.686.368 [mindspore/train/serialization.py:716] transformer.layers.23.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.687.176 [mindspore/train/serialization.py:716] transformer.layers.23.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.687.977 [mindspore/train/serialization.py:716] transformer.layers.24.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.688.802 [mindspore/train/serialization.py:716] transformer.layers.24.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.689.618 [mindspore/train/serialization.py:716] transformer.layers.25.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.690.408 [mindspore/train/serialization.py:716] transformer.layers.25.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.691.205 [mindspore/train/serialization.py:716] transformer.layers.26.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.691.994 [mindspore/train/serialization.py:716] transformer.layers.26.value_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.692.790 [mindspore/train/serialization.py:716] transformer.layers.27.key_past is not loaded.\n",
      "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.693.595 [mindspore/train/serialization.py:716] transformer.layers.27.value_past is not loaded.\n"
     ]
    }
   ],
   "source": [
    "ms.load_checkpoint(args.checkpoint_path, model)\n",
    "tokenizer = ChatGLMTokenizer(args.vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9526fb-0038-44c8-9032-8318f7e40f2e",
   "metadata": {},
   "source": [
    "此处是新增加的LoRA权重\n",
    "因此，出现\n",
    "[WARNING] ME(838466:281473066355264,MainProcess):2023-09-09-16:12:11.693.595 [mindspore/train/serialization.py:716] transformer.layers.27.value_past is not loaded.\n",
    "是正常现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c50f27-aa37-4887-b68b-0c6d42ce44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\"你好\", \"请介绍一下华为\", \"用Python写一个快排\"]\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8152f24d-a12c-4315-be79-e852e6d09f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_decode_length=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c021cc4-45ba-443d-97d5-3389009f2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query,history=history):\n",
    "    if not history:\n",
    "        prompt = query\n",
    "    else:\n",
    "        prompt = \"\"\n",
    "        for i, (old_query, response) in enumerate(history):\n",
    "            prompt += \"[Round {}]\\n问：{}\\n答：{}\\n\".format(i, old_query, response)\n",
    "        prompt += \"[Round {}]\\n问：{}\\n答：\".format(len(history), query)\n",
    "    inputs = tokenizer(prompt)\n",
    "\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(np.expand_dims(np.array(inputs['input_ids']).astype(np.int32), 0),\n",
    "                             max_length=config.max_decode_length, do_sample=False, top_p=0.7, top_k=1)\n",
    "    end_time = time.time()\n",
    "    print(f'generate speed: {outputs[0].shape[0]/(end_time-start_time):.2f} tokens/s')\n",
    "    response = tokenizer.decode(outputs)\n",
    "    response = process_response(response[0])\n",
    "    history = history + [(query, response)]\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb808fa-972e-4954-9e02-2db67178aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(\"您上海迪士尼乐园的会员卡号是?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
